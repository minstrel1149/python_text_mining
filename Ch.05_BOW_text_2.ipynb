{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ef5586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e37fe8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4efab40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
      "{np.int64(0), np.int64(1), np.int64(2), np.int64(3)}\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.target_names)\n",
    "print(set(newsgroups_train.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c5d2bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = newsgroups_train.data, newsgroups_train.target, newsgroups_test.data, newsgroups_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2468c58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 2000) (1353, 2000)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_features=2000, min_df=3, max_df=0.5)\n",
    "X_train_tf = tfidf.fit_transform(X_train)\n",
    "X_test_tf = tfidf.transform(X_test)\n",
    "print(X_train_tf.shape, X_test_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8b92e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 : 0.0, 000 : 0.0, 01 : 0.0, 04 : 0.0, 05 : 0.0, 10 : 0.0, 100 : 0.0, 1000 : 0.0, 11 : 0.0, 12 : 0.0, 128 : 0.0, 129 : 0.0, 13 : 0.0, 130 : 0.0, 14 : 0.0, 15 : 0.0, 16 : 0.0, 17 : 0.0, 18 : 0.0, 19 : 0.0, 1988 : 0.0, 1989 : 0.0, 1990 : 0.0, 1991 : 0.0, 1992 : 0.0, 1993 : 0.0, 20 : 0.0, 200 : 0.0, 202 : 0.0, 21 : 0.0, 22 : 0.0, 23 : 0.0, 24 : 0.0, 25 : 0.0, 256 : 0.0, 26 : 0.0, 27 : 0.0, 28 : 0.0, 2d : 0.0, 30 : 0.0, 300 : 0.0, 31 : 0.0, 32 : 0.0, 33 : 0.0, 34 : 0.0, 35 : 0.0, 39 : 0.0, 3d : 0.0, 40 : 0.0, 400 : 0.0, "
     ]
    }
   ],
   "source": [
    "for word, count in zip(tfidf.get_feature_names_out()[:50], X_train_tf[0].toarray()[0, :50]):\n",
    "    print(word, ':', count, end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc842cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8623402163225172\n",
      "0.7390983000739099\n"
     ]
    }
   ],
   "source": [
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(X_train_tf, y_train)\n",
    "print(nb_clf.score(X_train_tf, y_train))\n",
    "print(nb_clf.score(X_test_tf, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "204b4974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sci.space\n",
      "comp.graphics\n",
      "comp.graphics\n",
      "comp.graphics\n",
      "comp.graphics\n",
      "comp.graphics\n",
      "sci.space\n",
      "sci.space\n",
      "alt.atheism\n",
      "sci.space\n"
     ]
    }
   ],
   "source": [
    "pred = nb_clf.predict(X_test_tf[:10])\n",
    "for i in pred:\n",
    "    print(newsgroups_train.target_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaadebc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 5000) (1353, 5000)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(token_pattern=r\"[\\w']{2,}\", lowercase=True, max_features=5000, min_df=3, max_df=0.5)\n",
    "X_train_tf = tfidf.fit_transform(X_train)\n",
    "X_test_tf = tfidf.transform(X_test)\n",
    "print(X_train_tf.shape, X_test_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "464a2674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9444444444444444\n",
      "0.7753141167775314\n"
     ]
    }
   ],
   "source": [
    "nb_clf = MultinomialNB(alpha=0.1)\n",
    "nb_clf.fit(X_train_tf, y_train)\n",
    "print(nb_clf.score(X_train_tf, y_train))\n",
    "print(nb_clf.score(X_test_tf, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f2a094a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: sci.space / y_test: sci.space\n",
      "pred: comp.graphics / y_test: comp.graphics\n",
      "pred: comp.graphics / y_test: comp.graphics\n",
      "pred: comp.graphics / y_test: comp.graphics\n",
      "pred: comp.graphics / y_test: comp.graphics\n",
      "pred: comp.graphics / y_test: comp.graphics\n",
      "pred: sci.space / y_test: sci.space\n",
      "pred: sci.space / y_test: sci.space\n",
      "pred: alt.atheism / y_test: alt.atheism\n",
      "pred: sci.space / y_test: sci.space\n"
     ]
    }
   ],
   "source": [
    "pred = nb_clf.predict(X_test_tf[:10])\n",
    "for i, j in zip(pred, y_test):\n",
    "    print('pred:', newsgroups_train.target_names[i], '/ y_test:', newsgroups_train.target_names[j], sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfb8a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_features(classifier, vectorizer, categories, n):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names_out())\n",
    "    for i, category in enumerate(categories):\n",
    "        if isinstance(classifier, MultinomialNB):\n",
    "            top_n = np.argsort(-classifier.feature_count_[i])[:n]\n",
    "            print(f'{category}: {', '.join(feature_names[top_n])}')\n",
    "        else:\n",
    "            top_n = np.argsort(-classifier.coef_[i])[:n]\n",
    "            print(f'{category}: {', '.join(feature_names[top_n])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "795b676b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: you, not, are, be, this, have, as, what, if, they, do, god, but, your, or, so, was, on, an, we\n",
      "comp.graphics: you, graphics, on, this, have, any, or, thanks, with, if, can, be, but, there, image, are, files, file, me, anyone\n",
      "sci.space: space, on, be, was, you, this, as, are, have, they, at, would, or, if, from, not, but, with, by, nasa\n",
      "talk.religion.misc: you, not, he, are, this, as, be, was, god, they, have, with, your, but, who, jesus, or, by, his, what\n"
     ]
    }
   ],
   "source": [
    "top_n_features(nb_clf, tfidf, newsgroups_train.target_names, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e820aeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9680432645034415 0.7627494456762749\n"
     ]
    }
   ],
   "source": [
    "logreg_clf = LogisticRegression(max_iter=1000, C=2)\n",
    "logreg_clf.fit(X_train_tf, y_train)\n",
    "print(logreg_clf.score(X_train_tf, y_train), logreg_clf.score(X_test_tf, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3af0cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: sci.space / y_test: sci.space\n",
      "pred: comp.graphics / y_test: comp.graphics\n",
      "pred: comp.graphics / y_test: comp.graphics\n",
      "pred: comp.graphics / y_test: comp.graphics\n",
      "pred: comp.graphics / y_test: comp.graphics\n",
      "pred: comp.graphics / y_test: comp.graphics\n",
      "pred: sci.space / y_test: sci.space\n",
      "pred: sci.space / y_test: sci.space\n",
      "pred: alt.atheism / y_test: alt.atheism\n",
      "pred: sci.space / y_test: sci.space\n"
     ]
    }
   ],
   "source": [
    "pred = logreg_clf.predict(X_test_tf[:10])\n",
    "for i, j in zip(pred, y_test):\n",
    "    print('pred:', newsgroups_train.target_names[i], '/ y_test:', newsgroups_train.target_names[j], sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7630bddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: atheism, religion, atheists, islam, bobby, deletion, islamic, motto, atheist, god, up, must, punishment, post, him, satan, people, cruel, bible, isn't\n",
      "comp.graphics: graphics, image, file, computer, 3d, files, hi, looking, points, code, format, package, video, 68070, anyone, images, screen, color, card, windows\n",
      "sci.space: space, orbit, nasa, launch, moon, spacecraft, shuttle, dc, lunar, solar, earth, flight, sci, mars, cost, get, satellite, like, at, year\n",
      "talk.religion.misc: christian, christians, god, jesus, he, objective, fbi, his, blood, christ, children, see, order, koresh, rosicrucian, who, amorc, abortion, kent, values\n"
     ]
    }
   ],
   "source": [
    "top_n_features(logreg_clf, tfidf, newsgroups_train.target_names, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d9f320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(text):\n",
    "    eng_stops = set(stopwords.words('english'))\n",
    "    reg_tokens = RegexpTokenizer(r\"[\\w']{2,}\").tokenize(text.lower())\n",
    "    words = [word for word in reg_tokens if (word not in eng_stops)]\n",
    "    lemma = WordNetLemmatizer()\n",
    "    tokens = [lemma.lemmatize(lemma_word) for lemma_word in words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21cb2bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Coding\\Local\\python_text_mining\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "my_tfidf = TfidfVectorizer(tokenizer=my_tokenizer, max_features=10000, min_df=3, max_df=0.5)\n",
    "X_train_my_tf = my_tfidf.fit_transform(X_train)\n",
    "X_test_my_tf = my_tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ad7bd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.956243854473943\n",
      "0.7871396895787139\n"
     ]
    }
   ],
   "source": [
    "my_nb_clf = MultinomialNB(alpha=0.1)\n",
    "my_nb_clf.fit(X_train_my_tf, y_train)\n",
    "print(my_nb_clf.score(X_train_my_tf, y_train))\n",
    "print(my_nb_clf.score(X_test_my_tf, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3604de1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: god, one, people, think, would, say, religion, atheist, atheism, could, thing, islam, know, argument, well, belief, make, said, system, believe\n",
      "comp.graphics: file, graphic, image, thanks, program, know, anyone, format, would, window, color, help, looking, need, please, hi, 3d, use, code, software\n",
      "sci.space: space, would, nasa, like, launch, orbit, one, year, get, moon, shuttle, think, could, time, cost, thing, satellite, much, earth, also\n",
      "talk.religion.misc: christian, god, jesus, people, would, one, say, think, bible, know, see, objective, child, believe, word, u, may, koresh, good, life\n"
     ]
    }
   ],
   "source": [
    "top_n_features(my_nb_clf, my_tfidf, newsgroups_train.target_names, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e0f61be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9749262536873157 0.7605321507760532\n"
     ]
    }
   ],
   "source": [
    "my_logreg_clf = LogisticRegression(max_iter=1000, C=5)\n",
    "my_logreg_clf.fit(X_train_my_tf, y_train)\n",
    "print(my_logreg_clf.score(X_train_my_tf, y_train), my_logreg_clf.score(X_test_my_tf, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9386876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: atheist, atheism, religion, deletion, motto, islam, islamic, bobby, post, god, bible, argument, satan, people, claim, must, punishment, text, define, right\n",
      "comp.graphics: graphic, image, file, 3d, computer, hi, anyone, looking, package, card, code, polygon, work, point, format, 42, 68070, video, algorithm, window\n",
      "sci.space: space, orbit, nasa, launch, spacecraft, moon, flight, shuttle, satellite, rocket, cost, mar, get, dc, star, earth, solar, data, idea, lunar\n",
      "talk.religion.misc: christian, god, jesus, child, fbi, objective, christ, order, blood, rosicrucian, story, mr, amorc, context, hudson, commandment, koresh, see, abortion, fire\n"
     ]
    }
   ],
   "source": [
    "top_n_features(my_logreg_clf, my_tfidf, newsgroups_train.target_names, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20c27248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 12959) (1353, 12959)\n"
     ]
    }
   ],
   "source": [
    "ngram_tfidf = TfidfVectorizer(token_pattern=r\"[\\w']{2,}\", lowercase=True, min_df=3, max_df=0.5, ngram_range=(1, 3), stop_words='english')\n",
    "X_train_ngram_tf = ngram_tfidf.fit_transform(X_train)\n",
    "X_test_ngram_tf = ngram_tfidf.transform(X_test)\n",
    "print(X_train_ngram_tf.shape, X_test_ngram_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "340ed048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9695181907571289\n",
      "0.7878787878787878\n"
     ]
    }
   ],
   "source": [
    "ngram_nb_clf = MultinomialNB(alpha=0.01)\n",
    "ngram_nb_clf.fit(X_train_ngram_tf, y_train)\n",
    "print(ngram_nb_clf.score(X_train_ngram_tf, y_train))\n",
    "print(ngram_nb_clf.score(X_test_ngram_tf, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bf3204e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.976401179941003 0.7612712490761271\n"
     ]
    }
   ],
   "source": [
    "ngram_logreg_clf = LogisticRegression(max_iter=1000, C=10)\n",
    "ngram_logreg_clf.fit(X_train_ngram_tf, y_train)\n",
    "print(ngram_logreg_clf.score(X_train_ngram_tf, y_train), ngram_logreg_clf.score(X_test_ngram_tf, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89a6f0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: god, people, don't, think, just, say, religion, atheism, does, islam, know, atheists, it's, bible, like, believe, i'm, said, time, true\n",
      "comp.graphics: graphics, thanks, image, files, file, know, program, does, looking, hi, i'm, need, format, use, windows, software, help, 3d, like, code\n",
      "sci.space: space, nasa, like, launch, just, orbit, moon, think, shuttle, earth, lunar, don't, time, know, data, people, spacecraft, cost, it's, year\n",
      "talk.religion.misc: god, jesus, people, christian, christians, don't, just, bible, think, know, objective, did, say, believe, good, koresh, does, life, christ, like\n",
      "None\n",
      "alt.atheism: atheism, religion, atheists, islam, deletion, atheist, motto, islamic, bible, bobby, post, loans, cruel, nanci, risk, you're right, people, punishment, perfect, define\n",
      "comp.graphics: graphics, image, file, computer, hi, 3d, looking, files, package, 68070, points, video, card, code, thanks, tiff, format, screen, 42, ftp\n",
      "sci.space: space, orbit, nasa, launch, spacecraft, moon, shuttle, dc, mars, solar, lunar, flight, sounds, earth, satellite, centaur, cost, data, like, funding\n",
      "talk.religion.misc: christian, god, christians, fbi, children, jesus, christ, objective, blood, wrong, rosicrucian, order, koresh, abortion, andreas, hudson, creation, ye, amorc, commandment\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(top_n_features(ngram_nb_clf, ngram_tfidf, newsgroups_train.target_names, 20))\n",
    "print(top_n_features(ngram_logreg_clf, ngram_tfidf, newsgroups_train.target_names, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ce775ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\n",
      "신과함께      4947\n",
      "택시운전사     2322\n",
      "인피니티 워    2042\n",
      "범죄도시      1939\n",
      "곤지암       1547\n",
      "라라랜드      1150\n",
      "코코         778\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>돈 들인건 티가 나지만 보는 내내 하품만</td>\n",
       "      <td>1</td>\n",
       "      <td>2018.10.29</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.26</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이전 작품에 비해 더 화려하고 스케일도 커졌지만.... 전국 맛집의 음식들을 한데 ...</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.24</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이 정도면 볼만하다고 할 수 있음!</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.22</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>재미있다</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.20</td>\n",
       "      <td>인피니티 워</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating        date  \\\n",
       "0                             돈 들인건 티가 나지만 보는 내내 하품만       1  2018.10.29   \n",
       "1       몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.      10  2018.10.26   \n",
       "2  이전 작품에 비해 더 화려하고 스케일도 커졌지만.... 전국 맛집의 음식들을 한데 ...       8  2018.10.24   \n",
       "3                                이 정도면 볼만하다고 할 수 있음!       8  2018.10.22   \n",
       "4                                               재미있다      10  2018.10.20   \n",
       "\n",
       "    title  \n",
       "0  인피니티 워  \n",
       "1  인피니티 워  \n",
       "2  인피니티 워  \n",
       "3  인피니티 워  \n",
       "4  인피니티 워  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/daum_movie_review.csv')\n",
    "print(df.title.value_counts())\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16d9f470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11043,), (3682,), (11043,), (3682,))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['title'], test_size=0.25, random_state=0)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d70eec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['몰입', '할수밖에', '없다', '.', '어렵게', '생각', '할', '필요없다', '.', '내', '가', '전투', '에', '참여', '한', '듯', '손', '에', '땀', '이남', '.']\n",
      "['몰입', '생각', '내', '전투', '참여', '듯', '손', '땀', '이남']\n",
      "[('몰입', 'Noun'), ('할수밖에', 'Verb'), ('없다', 'Adjective'), ('.', 'Punctuation'), ('어렵게', 'Adjective'), ('생각', 'Noun'), ('할', 'Verb'), ('필요없다', 'Adjective'), ('.', 'Punctuation'), ('내', 'Noun'), ('가', 'Josa'), ('전투', 'Noun'), ('에', 'Josa'), ('참여', 'Noun'), ('한', 'Determiner'), ('듯', 'Noun'), ('손', 'Noun'), ('에', 'Josa'), ('땀', 'Noun'), ('이남', 'Noun'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "okt = Okt()\n",
    "print(okt.morphs(X_train[1]))\n",
    "print(okt.nouns(X_train[1]))\n",
    "print(okt.pos(X_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d05b3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kor_tokenizer(text):\n",
    "    target_tags = ['Noun', 'Verb', 'Adjective']\n",
    "    result = ['/'.join([word, tag]) for word, tag in okt.pos(text, norm=True, stem=True) if tag in target_tags]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "640bc6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Coding\\Local\\python_text_mining\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "kor_tfidf = TfidfVectorizer(tokenizer=kor_tokenizer, min_df=3, max_df=0.5, ngram_range=(1, 2))\n",
    "X_train_kor_tf = kor_tfidf.fit_transform(X_train)\n",
    "X_test_kor_tf = kor_tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33783284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11043, 10897) (3682, 10897)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_kor_tf.shape, X_test_kor_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6e928ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8573757131214343 0.72270505160239\n"
     ]
    }
   ],
   "source": [
    "kor_nb_clf = MultinomialNB(alpha=0.1)\n",
    "kor_nb_clf.fit(X_train_kor_tf, y_train)\n",
    "print(kor_nb_clf.score(X_train_kor_tf, y_train), kor_nb_clf.score(X_test_kor_tf, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b9016ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9393280811373721 0.7175448126018468\n"
     ]
    }
   ],
   "source": [
    "kor_logreg_clf = LogisticRegression(max_iter=1000, C=10)\n",
    "kor_logreg_clf.fit(X_train_kor_tf, y_train)\n",
    "print(kor_logreg_clf.score(X_train_kor_tf, y_train), kor_logreg_clf.score(X_test_kor_tf, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2a9a97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
