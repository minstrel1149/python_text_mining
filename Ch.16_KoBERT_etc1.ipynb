{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_metric\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AdamW\n",
    "from kobert_tokenizer import KoBERTTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ëˆ ë“¤ì¸ê±´ í‹°ê°€ ë‚˜ì§€ë§Œ ë³´ëŠ” ë‚´ë‚´ í•˜í’ˆë§Œ</td>\n",
       "      <td>1</td>\n",
       "      <td>2018.10.29</td>\n",
       "      <td>ì¸í”¼ë‹ˆí‹° ì›Œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ë‚˜ëŠ” ì¬ë°Œê²Œ ë´„</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.14</td>\n",
       "      <td>ì¸í”¼ë‹ˆí‹° ì›Œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ê°œì—°ì„±ì€ ë¬´ì‹œí•´ë¼ ì•¡ì…˜ì„ ì¦ê²¨ë¼ ìŠ¤íƒ€ë¡œë“œê°€ ì´ëŒì–´ì¤€ë‹¤ ê°ê°ì˜ ì˜ì›…ë“¤ì„ ì¦ê²¨ë¼ ê·¸ë¦¬ê³ ...</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.01</td>\n",
       "      <td>ì¸í”¼ë‹ˆí‹° ì›Œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ë§ˆì§€ë§‰ì— ëˆ„êµ¬í•œí…Œ ì—°ë½í•œê±°ì§€? ê¶ê¸ˆ</td>\n",
       "      <td>9</td>\n",
       "      <td>2018.09.26</td>\n",
       "      <td>ì¸í”¼ë‹ˆí‹° ì›Œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ì´ì œëŠ” ì§€ê²¨ì›Œì„œ ëª»ë³´ê² ë‹¤</td>\n",
       "      <td>5</td>\n",
       "      <td>2018.09.26</td>\n",
       "      <td>ì¸í”¼ë‹ˆí‹° ì›Œ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating        date  \\\n",
       "0                             ëˆ ë“¤ì¸ê±´ í‹°ê°€ ë‚˜ì§€ë§Œ ë³´ëŠ” ë‚´ë‚´ í•˜í’ˆë§Œ       1  2018.10.29   \n",
       "1                                           ë‚˜ëŠ” ì¬ë°Œê²Œ ë´„      10  2018.10.14   \n",
       "2  ê°œì—°ì„±ì€ ë¬´ì‹œí•´ë¼ ì•¡ì…˜ì„ ì¦ê²¨ë¼ ìŠ¤íƒ€ë¡œë“œê°€ ì´ëŒì–´ì¤€ë‹¤ ê°ê°ì˜ ì˜ì›…ë“¤ì„ ì¦ê²¨ë¼ ê·¸ë¦¬ê³ ...       8  2018.10.01   \n",
       "3                                ë§ˆì§€ë§‰ì— ëˆ„êµ¬í•œí…Œ ì—°ë½í•œê±°ì§€? ê¶ê¸ˆ       9  2018.09.26   \n",
       "4                                      ì´ì œëŠ” ì§€ê²¨ì›Œì„œ ëª»ë³´ê² ë‹¤       5  2018.09.26   \n",
       "\n",
       "    title  \n",
       "0  ì¸í”¼ë‹ˆí‹° ì›Œ  \n",
       "1  ì¸í”¼ë‹ˆí‹° ì›Œ  \n",
       "2  ì¸í”¼ë‹ˆí‹° ì›Œ  \n",
       "3  ì¸í”¼ë‹ˆí‹° ì›Œ  \n",
       "4  ì¸í”¼ë‹ˆí‹° ì›Œ  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/daum_movie_review.csv')\n",
    "df = df.loc[::5, :].reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "      <th>soso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2940</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2941</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2942</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2944</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2945 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      bad  good  soso\n",
       "0     1.0   0.0   0.0\n",
       "1     0.0   1.0   0.0\n",
       "2     0.0   1.0   0.0\n",
       "3     0.0   1.0   0.0\n",
       "4     0.0   0.0   1.0\n",
       "...   ...   ...   ...\n",
       "2940  0.0   1.0   0.0\n",
       "2941  0.0   1.0   0.0\n",
       "2942  0.0   1.0   0.0\n",
       "2943  0.0   0.0   1.0\n",
       "2944  0.0   1.0   0.0\n",
       "\n",
       "[2945 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = (df\n",
    " .assign(result=lambda df: np.where(df['rating'] <= 4, 'bad', np.where(df['rating'] >= 8, 'good', 'soso')))\n",
    " ['result']\n",
    ")\n",
    "y = pd.get_dummies(y).astype('float')\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1545, 663, 737)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(df.review.tolist(), y.values, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.3, random_state=0)\n",
    "len(X_train), len(X_val), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_21144\\672663858.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('accuracy')\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metric(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.zeros_like(logits, dtype='float')\n",
    "    predictions[np.argmax(logits, axis=-1)] = float(1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        super().__init__()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at skt/kobert-base-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "model = BertForSequenceClassification.from_pretrained('skt/kobert-base-v1', num_labels=3, problem_type='multi_label_classification')\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "train_input = tokenizer(X_train, truncation=True, padding=True, return_tensors='pt')\n",
    "val_input = tokenizer(X_val, truncation=True, padding=True, return_tensors='pt')\n",
    "test_input = tokenizer(X_test, truncation=True, padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = OurDataset(train_input, y_train)\n",
    "val_dataset = OurDataset(val_input, y_val)\n",
    "test_dataset = OurDataset(test_input, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1545\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 194\n",
      "  Number of trainable parameters = 92189187\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9feb43485f654f78b9da478e61cdc9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_21144\\1541230738.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 663\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa6fcc4b05f4ff88b0faf82d4f07052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "only length-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\admin\\text_mining\\Ch.16_KoBERT_etc1.ipynb Cell 11\u001b[0m line \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/text_mining/Ch.16_KoBERT_etc1.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(output_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./results\u001b[39m\u001b[39m'\u001b[39m, num_train_epochs\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/text_mining/Ch.16_KoBERT_etc1.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                                   evaluation_strategy\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msteps\u001b[39m\u001b[39m'\u001b[39m, eval_steps\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/text_mining/Ch.16_KoBERT_etc1.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                   per_device_train_batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, per_device_eval_batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/text_mining/Ch.16_KoBERT_etc1.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                   warmup_steps\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, weight_decay\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/text_mining/Ch.16_KoBERT_etc1.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(model\u001b[39m=\u001b[39mmodel, args\u001b[39m=\u001b[39mtraining_args, train_dataset\u001b[39m=\u001b[39mtrain_dataset, eval_dataset\u001b[39m=\u001b[39mval_dataset, compute_metrics\u001b[39m=\u001b[39mcompute_metric)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/admin/text_mining/Ch.16_KoBERT_etc1.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1498\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1499\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1500\u001b[0m )\n\u001b[1;32m-> 1501\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1502\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1503\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1504\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1505\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1506\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:1826\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1823\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mepoch \u001b[39m=\u001b[39m epoch \u001b[39m+\u001b[39m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m steps_in_epoch\n\u001b[0;32m   1824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m-> 1826\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[0;32m   1827\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1828\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_substep_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2089\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2083\u001b[0m             metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate(\n\u001b[0;32m   2084\u001b[0m                 eval_dataset\u001b[39m=\u001b[39meval_dataset,\n\u001b[0;32m   2085\u001b[0m                 ignore_keys\u001b[39m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2086\u001b[0m                 metric_key_prefix\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meval_\u001b[39m\u001b[39m{\u001b[39;00meval_dataset_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2087\u001b[0m             )\n\u001b[0;32m   2088\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2089\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(ignore_keys\u001b[39m=\u001b[39;49mignore_keys_for_eval)\n\u001b[0;32m   2090\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2092\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_save:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2796\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2793\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   2795\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 2796\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[0;32m   2797\u001b[0m     eval_dataloader,\n\u001b[0;32m   2798\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   2799\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   2800\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   2801\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   2802\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[0;32m   2803\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[0;32m   2804\u001b[0m )\n\u001b[0;32m   2806\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[0;32m   2807\u001b[0m output\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mupdate(\n\u001b[0;32m   2808\u001b[0m     speed_metrics(\n\u001b[0;32m   2809\u001b[0m         metric_key_prefix,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2813\u001b[0m     )\n\u001b[0;32m   2814\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:3081\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3077\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics(\n\u001b[0;32m   3078\u001b[0m             EvalPrediction(predictions\u001b[39m=\u001b[39mall_preds, label_ids\u001b[39m=\u001b[39mall_labels, inputs\u001b[39m=\u001b[39mall_inputs)\n\u001b[0;32m   3079\u001b[0m         )\n\u001b[0;32m   3080\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 3081\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics(EvalPrediction(predictions\u001b[39m=\u001b[39;49mall_preds, label_ids\u001b[39m=\u001b[39;49mall_labels))\n\u001b[0;32m   3082\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3083\u001b[0m     metrics \u001b[39m=\u001b[39m {}\n",
      "\u001b[1;32mc:\\Users\\admin\\text_mining\\Ch.16_KoBERT_etc1.ipynb Cell 11\u001b[0m line \u001b[0;36mcompute_metric\u001b[1;34m(eval_pred)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/text_mining/Ch.16_KoBERT_etc1.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m logits, labels \u001b[39m=\u001b[39m eval_pred\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/text_mining/Ch.16_KoBERT_etc1.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m predictions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(logits, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/admin/text_mining/Ch.16_KoBERT_etc1.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mreturn\u001b[39;00m metric\u001b[39m.\u001b[39;49mcompute(predictions\u001b[39m=\u001b[39;49mpredictions, references\u001b[39m=\u001b[39;49mlabels)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\datasets\\metric.py:442\u001b[0m, in \u001b[0;36mMetric.compute\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    439\u001b[0m compute_kwargs \u001b[39m=\u001b[39m {k: kwargs[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m kwargs \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures}\n\u001b[0;32m    441\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m--> 442\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_batch(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m    443\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_finalize()\n\u001b[0;32m    445\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_file_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\datasets\\metric.py:494\u001b[0m, in \u001b[0;36mMetric.add_batch\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    492\u001b[0m batch \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpredictions\u001b[39m\u001b[39m\"\u001b[39m: predictions, \u001b[39m\"\u001b[39m\u001b[39mreferences\u001b[39m\u001b[39m\"\u001b[39m: references, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[0;32m    493\u001b[0m batch \u001b[39m=\u001b[39m {intput_name: batch[intput_name] \u001b[39mfor\u001b[39;00m intput_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures}\n\u001b[1;32m--> 494\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfo\u001b[39m.\u001b[39;49mfeatures\u001b[39m.\u001b[39;49mencode_batch(batch)\n\u001b[0;32m    495\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_writer()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\datasets\\features\\features.py:1886\u001b[0m, in \u001b[0;36mFeatures.encode_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m   1884\u001b[0m \u001b[39mfor\u001b[39;00m key, column \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   1885\u001b[0m     column \u001b[39m=\u001b[39m cast_to_python_objects(column)\n\u001b[1;32m-> 1886\u001b[0m     encoded_batch[key] \u001b[39m=\u001b[39m [encode_nested_example(\u001b[39mself\u001b[39m[key], obj) \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m column]\n\u001b[0;32m   1887\u001b[0m \u001b[39mreturn\u001b[39;00m encoded_batch\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\datasets\\features\\features.py:1886\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1884\u001b[0m \u001b[39mfor\u001b[39;00m key, column \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   1885\u001b[0m     column \u001b[39m=\u001b[39m cast_to_python_objects(column)\n\u001b[1;32m-> 1886\u001b[0m     encoded_batch[key] \u001b[39m=\u001b[39m [encode_nested_example(\u001b[39mself\u001b[39;49m[key], obj) \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m column]\n\u001b[0;32m   1887\u001b[0m \u001b[39mreturn\u001b[39;00m encoded_batch\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\datasets\\features\\features.py:1284\u001b[0m, in \u001b[0;36mencode_nested_example\u001b[1;34m(schema, obj, level)\u001b[0m\n\u001b[0;32m   1281\u001b[0m \u001b[39m# Object with special encoding:\u001b[39;00m\n\u001b[0;32m   1282\u001b[0m \u001b[39m# ClassLabel will convert from string to int, TranslationVariableLanguages does some checks\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (Audio, Image, ClassLabel, TranslationVariableLanguages, Value, _ArrayXD)):\n\u001b[1;32m-> 1284\u001b[0m     \u001b[39mreturn\u001b[39;00m schema\u001b[39m.\u001b[39;49mencode_example(obj) \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[39m# Other object should be directly convertible to a native Arrow type (like Translation and Translation)\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\datasets\\features\\features.py:513\u001b[0m, in \u001b[0;36mValue.encode_example\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(value)\n\u001b[0;32m    512\u001b[0m \u001b[39melif\u001b[39;00m pa\u001b[39m.\u001b[39mtypes\u001b[39m.\u001b[39mis_integer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpa_type):\n\u001b[1;32m--> 513\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39;49m(value)\n\u001b[0;32m    514\u001b[0m \u001b[39melif\u001b[39;00m pa\u001b[39m.\u001b[39mtypes\u001b[39m.\u001b[39mis_floating(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpa_type):\n\u001b[0;32m    515\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mfloat\u001b[39m(value)\n",
      "\u001b[1;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(output_dir='./results', num_train_epochs=2,\n",
    "                                  evaluation_strategy='steps', eval_steps=50,\n",
    "                                  per_device_train_batch_size=16, per_device_eval_batch_size=16,\n",
    "                                  warmup_steps=100, weight_decay=0.01)\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset, compute_metrics=compute_metric)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 737\n",
      "  Batch size = 16\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_16488\\1541230738.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argmax() got an unexpected keyword argument 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\admin\\text_mining\\Ch.16_KoBERT_etc1.ipynb Cell 12\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/admin/text_mining/Ch.16_KoBERT_etc1.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mevaluate(eval_dataset\u001b[39m=\u001b[39;49mtest_dataset)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2796\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2793\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   2795\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 2796\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[0;32m   2797\u001b[0m     eval_dataloader,\n\u001b[0;32m   2798\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   2799\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   2800\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   2801\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   2802\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[0;32m   2803\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[0;32m   2804\u001b[0m )\n\u001b[0;32m   2806\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[0;32m   2807\u001b[0m output\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mupdate(\n\u001b[0;32m   2808\u001b[0m     speed_metrics(\n\u001b[0;32m   2809\u001b[0m         metric_key_prefix,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2813\u001b[0m     )\n\u001b[0;32m   2814\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:3081\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3077\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics(\n\u001b[0;32m   3078\u001b[0m             EvalPrediction(predictions\u001b[39m=\u001b[39mall_preds, label_ids\u001b[39m=\u001b[39mall_labels, inputs\u001b[39m=\u001b[39mall_inputs)\n\u001b[0;32m   3079\u001b[0m         )\n\u001b[0;32m   3080\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 3081\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics(EvalPrediction(predictions\u001b[39m=\u001b[39;49mall_preds, label_ids\u001b[39m=\u001b[39;49mall_labels))\n\u001b[0;32m   3082\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3083\u001b[0m     metrics \u001b[39m=\u001b[39m {}\n",
      "\u001b[1;32mc:\\Users\\admin\\text_mining\\Ch.16_KoBERT_etc1.ipynb Cell 12\u001b[0m line \u001b[0;36mcompute_metric\u001b[1;34m(eval_pred)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/text_mining/Ch.16_KoBERT_etc1.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_metric\u001b[39m(eval_pred):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/text_mining/Ch.16_KoBERT_etc1.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     logits, labels \u001b[39m=\u001b[39m eval_pred\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/admin/text_mining/Ch.16_KoBERT_etc1.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     predictions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49margmax(logits, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/admin/text_mining/Ch.16_KoBERT_etc1.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m metric\u001b[39m.\u001b[39mcompute(predictions\u001b[39m=\u001b[39mpredictions, references\u001b[39m=\u001b[39mlabels)\n",
      "\u001b[1;31mTypeError\u001b[0m: argmax() got an unexpected keyword argument 'dim'"
     ]
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_a = np.array([0.1, 0.7, 0.2])\n",
    "np.argmax(logits_a, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros_like(logits_a)[np.argmax(logits_a, axis=-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.zeros_like(logits_a, dtype='float')\n",
    "pred[np.argmax(logits_a, axis=-1)] = float(1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
