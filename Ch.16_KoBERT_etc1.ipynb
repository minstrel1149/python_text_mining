{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_metric\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AdamW\n",
    "from kobert_tokenizer import KoBERTTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ëˆ ë“¤ì¸ê±´ í‹°ê°€ ë‚˜ì§€ë§Œ ë³´ëŠ” ë‚´ë‚´ í•˜í’ˆë§Œ</td>\n",
       "      <td>1</td>\n",
       "      <td>2018.10.29</td>\n",
       "      <td>ì¸í”¼ë‹ˆí‹° ì›Œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ë‚˜ëŠ” ì¬ë°Œê²Œ ë´„</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.14</td>\n",
       "      <td>ì¸í”¼ë‹ˆí‹° ì›Œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ê°œì—°ì„±ì€ ë¬´ì‹œí•´ë¼ ì•¡ì…˜ì„ ì¦ê²¨ë¼ ìŠ¤íƒ€ë¡œë“œê°€ ì´ëŒì–´ì¤€ë‹¤ ê°ê°ì˜ ì˜ì›…ë“¤ì„ ì¦ê²¨ë¼ ê·¸ë¦¬ê³ ...</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.01</td>\n",
       "      <td>ì¸í”¼ë‹ˆí‹° ì›Œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ë§ˆì§€ë§‰ì— ëˆ„êµ¬í•œí…Œ ì—°ë½í•œê±°ì§€? ê¶ê¸ˆ</td>\n",
       "      <td>9</td>\n",
       "      <td>2018.09.26</td>\n",
       "      <td>ì¸í”¼ë‹ˆí‹° ì›Œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ì´ì œëŠ” ì§€ê²¨ì›Œì„œ ëª»ë³´ê² ë‹¤</td>\n",
       "      <td>5</td>\n",
       "      <td>2018.09.26</td>\n",
       "      <td>ì¸í”¼ë‹ˆí‹° ì›Œ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating        date  \\\n",
       "0                             ëˆ ë“¤ì¸ê±´ í‹°ê°€ ë‚˜ì§€ë§Œ ë³´ëŠ” ë‚´ë‚´ í•˜í’ˆë§Œ       1  2018.10.29   \n",
       "1                                           ë‚˜ëŠ” ì¬ë°Œê²Œ ë´„      10  2018.10.14   \n",
       "2  ê°œì—°ì„±ì€ ë¬´ì‹œí•´ë¼ ì•¡ì…˜ì„ ì¦ê²¨ë¼ ìŠ¤íƒ€ë¡œë“œê°€ ì´ëŒì–´ì¤€ë‹¤ ê°ê°ì˜ ì˜ì›…ë“¤ì„ ì¦ê²¨ë¼ ê·¸ë¦¬ê³ ...       8  2018.10.01   \n",
       "3                                ë§ˆì§€ë§‰ì— ëˆ„êµ¬í•œí…Œ ì—°ë½í•œê±°ì§€? ê¶ê¸ˆ       9  2018.09.26   \n",
       "4                                      ì´ì œëŠ” ì§€ê²¨ì›Œì„œ ëª»ë³´ê² ë‹¤       5  2018.09.26   \n",
       "\n",
       "    title  \n",
       "0  ì¸í”¼ë‹ˆí‹° ì›Œ  \n",
       "1  ì¸í”¼ë‹ˆí‹° ì›Œ  \n",
       "2  ì¸í”¼ë‹ˆí‹° ì›Œ  \n",
       "3  ì¸í”¼ë‹ˆí‹° ì›Œ  \n",
       "4  ì¸í”¼ë‹ˆí‹° ì›Œ  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/daum_movie_review.csv')\n",
    "df = df.loc[::5, :].reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       2\n",
       "2       2\n",
       "3       2\n",
       "4       1\n",
       "       ..\n",
       "2940    2\n",
       "2941    2\n",
       "2942    2\n",
       "2943    1\n",
       "2944    2\n",
       "Name: result, Length: 2945, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = (df\n",
    " .assign(result=lambda df: np.where(df['rating'] <= 4, 'bad', np.where(df['rating'] >= 8, 'good', 'soso')))\n",
    " ['result']\n",
    " .replace({'bad':0, 'soso':1, 'good':2})\n",
    ")\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1545, 663, 737)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(df.review.tolist(), y.values, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.3, random_state=0)\n",
    "len(X_train), len(X_val), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_4256\\672663858.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('accuracy')\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metric(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        super().__init__()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at skt/kobert-base-v1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "model = BertForSequenceClassification.from_pretrained('skt/kobert-base-v1', num_labels=3)\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "train_input = tokenizer(X_train, truncation=True, padding=True, return_tensors='pt')\n",
    "val_input = tokenizer(X_val, truncation=True, padding=True, return_tensors='pt')\n",
    "test_input = tokenizer(X_test, truncation=True, padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = OurDataset(train_input, y_train)\n",
    "val_dataset = OurDataset(val_input, y_val)\n",
    "test_dataset = OurDataset(test_input, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1545\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 194\n",
      "  Number of trainable parameters = 92189187\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2a3ca66dca4736882ef755c388c8a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_4256\\1541230738.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 663\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7612f57e25c54097a683e53ae3ffa358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8689894676208496, 'eval_accuracy': 0.6636500754147813, 'eval_runtime': 355.1912, 'eval_samples_per_second': 1.867, 'eval_steps_per_second': 0.118, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_4256\\1541230738.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 663\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3fcbc161ca4baa97a31340ee3e10f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8431844115257263, 'eval_accuracy': 0.6621417797888386, 'eval_runtime': 341.2687, 'eval_samples_per_second': 1.943, 'eval_steps_per_second': 0.123, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_4256\\1541230738.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 663\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820af53c5ddc4c75a5a68b6091fb361a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7212477326393127, 'eval_accuracy': 0.6998491704374057, 'eval_runtime': 334.7483, 'eval_samples_per_second': 1.981, 'eval_steps_per_second': 0.125, 'epoch': 1.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_4256\\1541230738.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 663\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3456dfb57848c8bd742617d0c07eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6862955689430237, 'eval_accuracy': 0.7088989441930619, 'eval_runtime': 347.3004, 'eval_samples_per_second': 1.909, 'eval_steps_per_second': 0.121, 'epoch': 1.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_4256\\1541230738.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 5978.2802, 'train_samples_per_second': 0.517, 'train_steps_per_second': 0.032, 'train_loss': 0.8000916943107683, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=194, training_loss=0.8000916943107683, metrics={'train_runtime': 5978.2802, 'train_samples_per_second': 0.517, 'train_steps_per_second': 0.032, 'train_loss': 0.8000916943107683, 'epoch': 2.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(output_dir='./results', num_train_epochs=2,\n",
    "                                  evaluation_strategy='steps', eval_steps=40,\n",
    "                                  per_device_train_batch_size=16, per_device_eval_batch_size=16,\n",
    "                                  warmup_steps=100, weight_decay=0.01)\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset, compute_metrics=compute_metric)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 737\n",
      "  Batch size = 16\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_4256\\1541230738.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f217cde801bd444eb2e39e8a7084fb4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6582435965538025,\n",
       " 'eval_accuracy': 0.723202170963365,\n",
       " 'eval_runtime': 444.0189,\n",
       " 'eval_samples_per_second': 1.66,\n",
       " 'eval_steps_per_second': 0.106,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 737\n",
      "  Batch size = 16\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_4256\\1541230738.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b436437d7b54ddc92c64e74897a52ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 0.08356023, -0.34667933,  0.34367242],\n",
       "       [-1.3751858 , -1.039848  ,  2.2063167 ],\n",
       "       [-1.2503979 , -0.77620715,  2.0081325 ],\n",
       "       ...,\n",
       "       [ 0.13815361, -0.16921851,  0.06255788],\n",
       "       [-0.8249176 , -0.6109066 ,  1.369531  ],\n",
       "       [-1.4625283 , -0.8776499 ,  2.1051786 ]], dtype=float32), label_ids=array([0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 1, 0, 0, 2, 2, 2, 2,\n",
       "       0, 0, 1, 0, 2, 2, 2, 0, 0, 2, 1, 1, 0, 2, 0, 2, 2, 2, 2, 1, 2, 2,\n",
       "       2, 0, 2, 2, 1, 2, 2, 2, 2, 2, 0, 1, 0, 0, 2, 2, 2, 2, 1, 2, 2, 2,\n",
       "       0, 1, 0, 0, 1, 0, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2,\n",
       "       2, 2, 0, 0, 0, 2, 2, 0, 1, 2, 0, 0, 2, 2, 2, 0, 2, 0, 2, 2, 2, 0,\n",
       "       2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 1, 0, 2, 1, 2, 2, 2, 2, 0, 1, 0,\n",
       "       2, 2, 1, 2, 0, 0, 2, 2, 0, 2, 1, 2, 2, 0, 1, 0, 2, 2, 2, 1, 1, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 0, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0,\n",
       "       2, 0, 2, 2, 1, 2, 2, 1, 2, 2, 0, 2, 0, 1, 2, 2, 2, 2, 2, 0, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 1, 2, 2, 2, 2, 0, 2, 2, 0, 0, 1,\n",
       "       2, 2, 0, 2, 2, 2, 2, 1, 0, 0, 0, 2, 2, 1, 0, 1, 2, 1, 2, 2, 2, 2,\n",
       "       2, 2, 2, 0, 2, 2, 1, 0, 0, 2, 2, 0, 0, 2, 0, 2, 2, 2, 2, 0, 2, 2,\n",
       "       0, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 1, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 0, 1, 0, 2, 2, 2, 2, 0, 0, 1, 2, 1, 0, 0, 2, 0, 2, 2,\n",
       "       2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2, 2, 0, 2, 2,\n",
       "       1, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2,\n",
       "       1, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2,\n",
       "       0, 2, 1, 2, 2, 2, 0, 0, 1, 1, 2, 1, 1, 0, 0, 2, 0, 1, 0, 2, 1, 1,\n",
       "       0, 0, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2,\n",
       "       2, 0, 1, 2, 0, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 0, 1, 2, 0, 2, 2, 2,\n",
       "       2, 2, 2, 2, 0, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 1, 1, 1,\n",
       "       0, 2, 1, 1, 0, 2, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2,\n",
       "       2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 1, 1, 2, 2, 2, 1, 2, 2, 0, 2, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 0, 1, 0, 0, 1, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2,\n",
       "       2, 0, 2, 2, 2, 0, 2, 0, 2, 2, 2, 1, 2, 1, 0, 2, 2, 1, 0, 2, 1, 1,\n",
       "       2, 1, 0, 2, 0, 0, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 0, 2,\n",
       "       2, 2, 2, 0, 1, 2, 2, 1, 2, 2, 0, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0,\n",
       "       2, 2, 0, 0, 1, 2, 0, 0, 0, 2, 2, 0, 0, 2, 2, 2, 1, 0, 0, 2, 0, 1,\n",
       "       0, 2, 0, 1, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 1, 2, 2, 0, 2, 0,\n",
       "       2, 2, 2, 2, 2, 0, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 0, 0, 2, 2, 2, 2,\n",
       "       2, 2, 2, 1, 2, 0, 2, 2, 2, 1, 2, 1, 2, 0, 2, 2, 0, 2, 1, 2, 2, 2,\n",
       "       2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 1, 2, 2, 2, 0, 2, 1, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 1, 2, 1, 2, 2, 0, 2, 1, 2, 2,\n",
       "       2, 2, 0, 1, 2, 1, 0, 2, 2, 2, 2], dtype=int64), metrics={'test_loss': 0.6582435965538025, 'test_accuracy': 0.723202170963365, 'test_runtime': 445.8939, 'test_samples_per_second': 1.653, 'test_steps_per_second': 0.105})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = trainer.predict(test_dataset=test_dataset)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(737,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.label_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
