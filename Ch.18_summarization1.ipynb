{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline('summarization')\n",
    "text = '''Text mining, also referred to as text data mining (abbr.: TDM), similar to text analytics, \n",
    "        is the process of deriving high-quality information from text. It involves \n",
    "        \"the discovery by computer of new, previously unknown information, \n",
    "        by automatically extracting information from different written resources.\" \n",
    "        Written resources may include websites, books, emails, reviews, and articles. \n",
    "        High-quality information is typically obtained by devising patterns and trends \n",
    "        by means such as statistical pattern learning. According to Hotho et al. (2005)\n",
    "        we can distinguish between three different perspectives of text mining: \n",
    "        information extraction, data mining, and a KDD (Knowledge Discovery in Databases) process.''' \n",
    "result = summarizer(text)\n",
    "result[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('t5-small', model_max_length=512)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('t5-small')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokenizer), type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_text = text.strip().replace('\\n', '')\n",
    "input_text = 'summarize: ' + preprocess_text\n",
    "tokenized_text = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "summary_ids = model.generate(tokenized_text, num_beams=4, no_repeat_ngram_size=3,\n",
    "                             min_length=30, max_length=200, early_stopping=True)\n",
    "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text, summary_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'translate english to german: That is good'\n",
    "tokenized_text = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "summary_ids = model.generate(tokenized_text, num_beams=4, no_repeat_ngram_size=3,\n",
    "                             max_length=200, early_stopping=True)\n",
    "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5TokenizerFast.from_pretrained('t5-small', model_max_length=1024)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''The Inflation Reduction Act lowers prescription drug costs, health care costs, \n",
    "and energy costs. It's the most aggressive action on tackling the climate crisis in American history, \n",
    "which will lift up American workers and create good-paying, union jobs across the country. \n",
    "It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. \n",
    "And no one making under $400,000 per year will pay a penny more in taxes.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_text = text.strip().replace('\\n', '')\n",
    "input_text = 'summarize: ' + preprocess_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "summary_ids = model.generate(tokenized_text,\n",
    "                             num_beams=4, no_repeat_ngram_size=3,\n",
    "                             min_length=30, max_length=100, early_stopping=True)\n",
    "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum = load_dataset(\"billsum\", split=\"ca_test\")\n",
    "billsum = billsum.train_test_split(test_size=0.2)\n",
    "example = billsum[\"train\"][0]\n",
    "for key in example.keys():\n",
    "    print(key, '\\n', example[key])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(data):\n",
    "    inputs = [\"summarize: \" + doc for doc in data[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "    labels = tokenizer(data[\"summary\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_billsum = billsum.map(preprocess_text, batched=True, remove_columns=billsum[\"train\"].column_names)\n",
    "tokenized_billsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./summary\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    ")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_billsum[\"train\"],\n",
    "    eval_dataset=tokenized_billsum[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ids = model.generate(tokenized_text,\n",
    "                             num_beams=4, no_repeat_ngram_size=3,\n",
    "                             min_length=30, max_length=100, early_stopping=True)\n",
    "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"디아블로는 액션 롤플레잉 핵 앤드 슬래시 비디오 게임이다. \n",
    "플레이어는 주변 환경을 마우스로 사용해 영웅을 움직이게 한다. \n",
    "주문을 외는 등의 다른 활동은 키보드 입력으로 이루어진다. \n",
    "플레이어는 이 게임에서 장비를 획득하고, 주문을 배우고, 적을 쓰러뜨리며, NPC와 대화를 나눌 수 있다.\n",
    "지하 미궁은 주어진 형식이 있고 부분적으로 반복되는 형태가 존재하나 전체적으로 보면 무작위로 생성된다. \n",
    "예를 들어 지하 묘지의 경우에는 긴 복도와 닫힌 문들이 존재하고, 동굴은 좀 더 선형 형태를 띠고 있다. \n",
    "플레이어에게는 몇몇 단계에서 무작위의 퀘스트를 받는다. \n",
    "이 퀘스트는 선택적인 사항이나 플레이어의 영웅들을 성장시키거나 줄거리를 이해하는데 도움을 준다. \n",
    "그러나 맨 뒤에 두 퀘스트는 게임을 끝내기 위해 완료시켜야 한다.\"\"\"\n",
    "\n",
    "preprocess_text = text.strip().replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-summarization')\n",
    "model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-summarization')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.encode(preprocess_text, return_tensors=\"pt\")\n",
    "summary_ids = model.generate(tokenized_text,\n",
    "                             num_beams=4, no_repeat_ngram_size=3,\n",
    "                             min_length=10, max_length=150, early_stopping=True)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
