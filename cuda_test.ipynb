{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19680694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "   - GPU ì¥ì¹˜: NVIDIA GeForce RTX 5070\n",
      "==================================================\n",
      "[20000x20000] í–‰ë ¬ ê³±ì…ˆ ì†ë„ë¥¼ ë¹„êµí•©ë‹ˆë‹¤...\n",
      "   - ğŸ’» CPU ì—°ì‚° ì‹œê°„: 8.2219 ì´ˆ\n",
      "   - ğŸš€ GPU ì—°ì‚° ì‹œê°„: 0.0000 ì´ˆ\n",
      "==================================================\n",
      "ğŸ”¥ ì†ë„ ì°¨ì´: GPUê°€ CPUë³´ë‹¤ ì•½ 201667.7 ë°° ë¹ ë¦…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# --- 1. GPU ì„¤ì • í™•ì¸ ---\n",
    "if torch.cuda.is_available():\n",
    "    # ì‚¬ìš© ê°€ëŠ¥í•œ GPUê°€ ìˆëŠ”ì§€ í™•ì¸\n",
    "    print(\"âœ… GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ GPU ì¥ì¹˜ ë²ˆí˜¸ í™•ì¸\n",
    "    gpu_id = torch.cuda.current_device()\n",
    "    print(f\"   - GPU ì¥ì¹˜: {torch.cuda.get_device_name(gpu_id)}\")\n",
    "    \n",
    "    # ì‚¬ìš©í•  ì¥ì¹˜ë¥¼ 'cuda'ë¡œ ì„¤ì •\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "\n",
    "\n",
    "# --- 2. ì†ë„ ë¹„êµë¥¼ ìœ„í•œ ëŒ€ê·œëª¨ í…ì„œ(í–‰ë ¬) ìƒì„± ---\n",
    "# ë¹„êµí•  í…ì„œì˜ í¬ê¸° (ìˆ«ìë¥¼ í‚¤ìš¸ìˆ˜ë¡ ì°¨ì´ê°€ ê·¹ëª…í•´ì§‘ë‹ˆë‹¤)\n",
    "tensor_size = 20000\n",
    "\n",
    "# CPUì— í…ì„œ ìƒì„±\n",
    "cpu_tensor_a = torch.randn(tensor_size, tensor_size, device='cpu')\n",
    "cpu_tensor_b = torch.randn(tensor_size, tensor_size, device='cpu')\n",
    "\n",
    "# GPUì— ë™ì¼í•œ í…ì„œ ìƒì„± (GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ)\n",
    "if device.type == 'cuda':\n",
    "    gpu_tensor_a = cpu_tensor_a.to(device)\n",
    "    gpu_tensor_b = cpu_tensor_b.to(device)\n",
    "\n",
    "\n",
    "# --- 3. CPU ì—°ì‚° ì†ë„ ì¸¡ì • ---\n",
    "print(f\"[{tensor_size}x{tensor_size}] í–‰ë ¬ ê³±ì…ˆ ì†ë„ë¥¼ ë¹„êµí•©ë‹ˆë‹¤...\")\n",
    "\n",
    "start_time_cpu = time.time()\n",
    "result_cpu = torch.matmul(cpu_tensor_a, cpu_tensor_b)\n",
    "end_time_cpu = time.time()\n",
    "\n",
    "cpu_duration = end_time_cpu - start_time_cpu\n",
    "print(f\"   - ğŸ’» CPU ì—°ì‚° ì‹œê°„: {cpu_duration:.4f} ì´ˆ\")\n",
    "\n",
    "\n",
    "# --- 4. GPU ì—°ì‚° ì†ë„ ì¸¡ì • ---\n",
    "if device.type == 'cuda':\n",
    "    # GPU ì›Œë°ì—… (ì²« ì—°ì‚°ì€ ì´ˆê¸°í™” ë•Œë¬¸ì— ëŠë¦´ ìˆ˜ ìˆì–´ í•œë²ˆ ì‹¤í–‰í•´ ì¤Œ)\n",
    "    _ = torch.matmul(gpu_tensor_a, gpu_tensor_b)\n",
    "    \n",
    "    start_time_gpu = time.time()\n",
    "    result_gpu = torch.matmul(gpu_tensor_a, gpu_tensor_b)\n",
    "    #torch.cuda.synchronize() # âœ… ì´ ì¤„ì„ ì¶”ê°€í•˜ì—¬ CPUê°€ GPU ì—°ì‚°ì„ ê¸°ë‹¤ë¦¬ê²Œ í•¨\n",
    "    end_time_gpu = time.time()\n",
    "\n",
    "    gpu_duration = end_time_gpu - start_time_gpu\n",
    "    print(f\"   - ğŸš€ GPU ì—°ì‚° ì‹œê°„: {gpu_duration:.4f} ì´ˆ\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(f\"ğŸ”¥ ì†ë„ ì°¨ì´: GPUê°€ CPUë³´ë‹¤ ì•½ {cpu_duration / gpu_duration:.1f} ë°° ë¹ ë¦…ë‹ˆë‹¤.\")\n",
    "\n",
    "# ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” GPU í…ì„œ ë³€ìˆ˜ ì‚­ì œ\n",
    "del gpu_tensor_a\n",
    "del gpu_tensor_b\n",
    "del result_gpu\n",
    "\n",
    "# PyTorchê°€ VRAM ìºì‹œë¥¼ ë¹„ìš°ë„ë¡ ê°•ì œ\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0682f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
