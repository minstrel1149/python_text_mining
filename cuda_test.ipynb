{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19680694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU를 사용할 수 있습니다.\n",
      "   - GPU 장치: NVIDIA GeForce RTX 5070\n",
      "==================================================\n",
      "[20000x20000] 행렬 곱셈 속도를 비교합니다...\n",
      "   - 💻 CPU 연산 시간: 8.2219 초\n",
      "   - 🚀 GPU 연산 시간: 0.0000 초\n",
      "==================================================\n",
      "🔥 속도 차이: GPU가 CPU보다 약 201667.7 배 빠릅니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# --- 1. GPU 설정 확인 ---\n",
    "if torch.cuda.is_available():\n",
    "    # 사용 가능한 GPU가 있는지 확인\n",
    "    print(\"✅ GPU를 사용할 수 있습니다.\")\n",
    "    \n",
    "    # 현재 사용 중인 GPU 장치 번호 확인\n",
    "    gpu_id = torch.cuda.current_device()\n",
    "    print(f\"   - GPU 장치: {torch.cuda.get_device_name(gpu_id)}\")\n",
    "    \n",
    "    # 사용할 장치를 'cuda'로 설정\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ GPU를 사용할 수 없습니다. CPU를 사용합니다.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "\n",
    "\n",
    "# --- 2. 속도 비교를 위한 대규모 텐서(행렬) 생성 ---\n",
    "# 비교할 텐서의 크기 (숫자를 키울수록 차이가 극명해집니다)\n",
    "tensor_size = 20000\n",
    "\n",
    "# CPU에 텐서 생성\n",
    "cpu_tensor_a = torch.randn(tensor_size, tensor_size, device='cpu')\n",
    "cpu_tensor_b = torch.randn(tensor_size, tensor_size, device='cpu')\n",
    "\n",
    "# GPU에 동일한 텐서 생성 (GPU 사용 가능 시)\n",
    "if device.type == 'cuda':\n",
    "    gpu_tensor_a = cpu_tensor_a.to(device)\n",
    "    gpu_tensor_b = cpu_tensor_b.to(device)\n",
    "\n",
    "\n",
    "# --- 3. CPU 연산 속도 측정 ---\n",
    "print(f\"[{tensor_size}x{tensor_size}] 행렬 곱셈 속도를 비교합니다...\")\n",
    "\n",
    "start_time_cpu = time.time()\n",
    "result_cpu = torch.matmul(cpu_tensor_a, cpu_tensor_b)\n",
    "end_time_cpu = time.time()\n",
    "\n",
    "cpu_duration = end_time_cpu - start_time_cpu\n",
    "print(f\"   - 💻 CPU 연산 시간: {cpu_duration:.4f} 초\")\n",
    "\n",
    "\n",
    "# --- 4. GPU 연산 속도 측정 ---\n",
    "if device.type == 'cuda':\n",
    "    # GPU 워밍업 (첫 연산은 초기화 때문에 느릴 수 있어 한번 실행해 줌)\n",
    "    _ = torch.matmul(gpu_tensor_a, gpu_tensor_b)\n",
    "    \n",
    "    start_time_gpu = time.time()\n",
    "    result_gpu = torch.matmul(gpu_tensor_a, gpu_tensor_b)\n",
    "    #torch.cuda.synchronize() # ✅ 이 줄을 추가하여 CPU가 GPU 연산을 기다리게 함\n",
    "    end_time_gpu = time.time()\n",
    "\n",
    "    gpu_duration = end_time_gpu - start_time_gpu\n",
    "    print(f\"   - 🚀 GPU 연산 시간: {gpu_duration:.4f} 초\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(f\"🔥 속도 차이: GPU가 CPU보다 약 {cpu_duration / gpu_duration:.1f} 배 빠릅니다.\")\n",
    "\n",
    "# 더 이상 사용하지 않는 GPU 텐서 변수 삭제\n",
    "del gpu_tensor_a\n",
    "del gpu_tensor_b\n",
    "del result_gpu\n",
    "\n",
    "# PyTorch가 VRAM 캐시를 비우도록 강제\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0682f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
