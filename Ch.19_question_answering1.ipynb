{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizerFast\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DefaultDataCollator\n",
    "from transformers import ElectraForQuestionAnswering, ElectraTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = r'''Text mining, also referred to as text data mining (abbr.: TDM), similar to text analytics, \n",
    "        is the process of deriving high-quality information from text. It involves \n",
    "        \"the discovery by computer of new, previously unknown information, \n",
    "        by automatically extracting information from different written resources.\" \n",
    "        Written resources may include websites, books, emails, reviews, and articles. \n",
    "        High-quality information is typically obtained by devising patterns and trends \n",
    "        by means such as statistical pattern learning. According to Hotho et al. (2005)\n",
    "        we can distinguish between three different perspectives of text mining: \n",
    "        information extraction, data mining, and a KDD (Knowledge Discovery in Databases) process.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is text mining?\"\n",
    "question2 = \"What are the perspectives of text mining?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "model = AutoModelForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(question, context, return_tensors='pt').to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.models.distilbert.tokenization_distilbert_fast.DistilBertTokenizerFast,\n",
       " transformers.models.distilbert.modeling_distilbert.DistilBertForQuestionAnswering,\n",
       " transformers.modeling_outputs.QuestionAnsweringModelOutput)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer), type(model), type(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([35]), tensor([46]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_start_scores = outputs.start_logits\n",
    "answer_end_scores = outputs.end_logits\n",
    "answer_start = torch.argmax(answer_start_scores, dim=-1)\n",
    "answer_end = torch.argmax(answer_end_scores, dim=-1) + 1\n",
    "answer_start, answer_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is text mining? : the process of deriving high - quality information from text\n"
     ]
    }
   ],
   "source": [
    "input_ids = inputs['input_ids'].tolist()[0]\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "print(question, ':', answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"The city is the birthplace of many cultural movements, including the Harlem \n",
    "        Renaissance in literature and visual art; abstract expressionism \n",
    "        (also known as the New York School) in painting; and hip hop, punk, salsa, disco, \n",
    "        freestyle, Tin Pan Alley, and Jazz in music. New York City has been considered \n",
    "        the dance capital of the world. The city is also widely celebrated in popular lore, \n",
    "        frequently the setting for books, movies (see List of films set in New York City), \n",
    "        and television programs.\"\"\"\n",
    "question = \"The dance capital of the world is what city in the US?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '56ceddd1aab44d1400b88b58', 'title': 'Spectre_(2015_film)', 'context': 'In September 2015 it was announced that Sam Smith and regular collaborator Jimmy Napes had written the film\\'s title theme, \"Writing\\'s on the Wall\", with Smith performing it for the film. Smith said the song came together in one session and that he and Napes wrote it in under half an hour before recording a demo. Satisfied with the quality, the demo was used in the final release.', 'question': 'What was the name of the song played during the opening credits?', 'answers': {'text': [\"Writing's on the Wall\"], 'answer_start': [124]}}\n"
     ]
    }
   ],
   "source": [
    "squad = load_dataset('squad', split='train[:5000]')\n",
    "squad = squad.train_test_split(test_size=0.2)\n",
    "print(squad['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    questions = [q.strip() for q in data[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        data[\"context\"],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = data[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df81ae3bccc941d2892a031c953d84fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8948b6cf184abf8cbae3a1a7f1f279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_squad = squad.map(preprocess, batched=True, remove_columns=squad[\"train\"].column_names)\n",
    "tokenized_squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([124], [\"Writing's on the Wall\"])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad['train'][0]['answers']['answer_start'], squad['train'][0]['answers']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 250\n",
      "  Number of trainable parameters = 66364418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235d856bfb77403c91904d63628c2265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3537e5766d4ac4bbbf336be16f7664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.531589508056641, 'eval_runtime': 419.3744, 'eval_samples_per_second': 2.385, 'eval_steps_per_second': 0.15, 'epoch': 1.0}\n",
      "{'train_runtime': 4610.0212, 'train_samples_per_second': 0.868, 'train_steps_per_second': 0.054, 'train_loss': 5.36508154296875, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=5.36508154296875, metrics={'train_runtime': 4610.0212, 'train_samples_per_second': 0.868, 'train_steps_per_second': 0.054, 'train_loss': 5.36508154296875, 'epoch': 1.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./QandA\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_squad[\"train\"],\n",
    "    eval_dataset=tokenized_squad[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(question, context, return_tensors='pt').to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_start_logits = outputs.start_logits\n",
    "answer_end_logits = outputs.end_logits\n",
    "answer_start = torch.argmax(answer_start_logits)\n",
    "answer_end = torch.argmax(answer_end_logits) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dance capital of the world is what city in the US? : frequently\n"
     ]
    }
   ],
   "source": [
    "input_ids = inputs['input_ids'].tolist()[0]\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "print(question, ':', answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"수원 화성은 언제 완성되었는가?\"\n",
    "context = \"\"\"수원 화성은 조선시대 화성유수부 시가지를 둘러싼 성곽이다. \n",
    "1789년(정조 13) 수원을 팔달산 동쪽 아래로 옮기고, \n",
    "1794년(정조 18) 축성을 시작해 1796년에 완성했다.\"\"\"\n",
    "context = context.strip().replace(\"\\n\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--monologg--koelectra-small-v2-distilled-korquad-384\\snapshots\\256efd8763caf2d3936e141107f113e2ab51f653\\vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--monologg--koelectra-small-v2-distilled-korquad-384\\snapshots\\256efd8763caf2d3936e141107f113e2ab51f653\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--monologg--koelectra-small-v2-distilled-korquad-384\\snapshots\\256efd8763caf2d3936e141107f113e2ab51f653\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--monologg--koelectra-small-v2-distilled-korquad-384\\snapshots\\256efd8763caf2d3936e141107f113e2ab51f653\\config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"monologg/koelectra-small-v2-distilled-korquad-384\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32200\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--monologg--koelectra-small-v2-distilled-korquad-384\\snapshots\\256efd8763caf2d3936e141107f113e2ab51f653\\config.json\n",
      "Model config ElectraConfig {\n",
      "  \"architectures\": [\n",
      "    \"ElectraForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32200\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--monologg--koelectra-small-v2-distilled-korquad-384\\snapshots\\256efd8763caf2d3936e141107f113e2ab51f653\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraForQuestionAnswering.\n",
      "\n",
      "All the weights of ElectraForQuestionAnswering were initialized from the model checkpoint at monologg/koelectra-small-v2-distilled-korquad-384.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-small-v2-distilled-korquad-384\")\n",
    "model = ElectraForQuestionAnswering.from_pretrained(\"monologg/koelectra-small-v2-distilled-korquad-384\")\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(question, context, return_tensors='pt').to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_start_logits = outputs.start_logits\n",
    "answer_end_logits = outputs.end_logits\n",
    "answer_start = torch.argmax(answer_start_logits)\n",
    "answer_end = torch.argmax(answer_end_logits) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수원 화성은 언제 완성되었는가? : 1796년\n"
     ]
    }
   ],
   "source": [
    "input_ids = inputs['input_ids'].tolist()[0]\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "print(question, ':', answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
